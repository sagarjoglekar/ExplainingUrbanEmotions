{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:44:50.006301Z",
     "start_time": "2018-06-11T19:44:49.292449Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/sagarj/Work/caffe-fr-chairs/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n",
      "/work/sagarj/Work/caffe-fr-chairs/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n",
      "/work/sagarj/Work/caffe-fr-chairs/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "import caffe\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import (array, dot, arccos, clip)\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:44:50.793466Z",
     "start_time": "2018-06-11T19:44:50.782294Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Data/categoryIndex_places205.csv\" , 'rb') as f:\n",
    "    cats = f.readlines()\n",
    "catDict = {}\n",
    "for c in cats:\n",
    "    comps = c.strip().split(',')\n",
    "    idx = comps[0].split(' ')[1]\n",
    "    lbl = comps[0].split(' ')[0]\n",
    "    catDict[idx] = {}\n",
    "    catDict[idx]['label'] = lbl\n",
    "    catDict[idx]['tnomy'] = comps[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:44:52.213733Z",
     "start_time": "2018-06-11T19:44:52.204898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': '/p/palace', 'tnomy': 'L'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catDict[catDict.keys()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:44:53.219585Z",
     "start_time": "2018-06-11T19:44:52.901968Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Data/cityDf.pkl\" , 'rb') as f:\n",
    "    cityImages = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:44:58.211797Z",
     "start_time": "2018-06-11T19:44:58.205851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3164"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cityImages.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:45:00.812224Z",
     "start_time": "2018-06-11T19:45:00.805820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': '513d7b59fdc9f03587006b2f',\n",
       " 'label': 0,\n",
       " 'path': '/work/sagarj/Work/BellLabs/streetview/PPImages/513d7b59fdc9f03587006b2f.jpg',\n",
       " 'trueSkill': [18.990548645401752]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cityImages[cityImages.keys()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:45:04.283029Z",
     "start_time": "2018-06-11T19:45:04.276785Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YesLabels = ['/b/bridge', '/f/forest_road', '/v/viaduct', '/h/highway', '/f/forest_path', '/s/sky', '/t/tree_farm', '/r/rainforest', '/r/runway', '/r/railroad_track', '/m/mountain_snowy', '/w/wind_farm', '/b/bamboo_forest', '/o/orchard', '/r/racecourse', '/v/valley', '/f/field/wild', '/c/corn_field', '/t/track/outdoor', '/s/snowfield']\n",
    "NoLabels = ['/c/courthouse', '/f/formal_garden', '/h/hotel/outdoor', '/g/gas_station', '/b/building_facade', '/p/playground', '/a/alley', '/o/office_building', '/c/construction_site', '/r/residential_neighborhood', '/p/patio', '/d/driveway', '/m/motel', '/a/apartment_building/outdoor', '/h/hospital', '/p/plaza', '/m/mansion', '/y/yard', '/i/inn/outdoor', '/c/courtyard']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:45:14.709521Z",
     "start_time": "2018-06-11T19:45:14.612329Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you get \"No module named _caffe\", either you have not built pycaffe or you have the wrong path.\n",
    "\n",
    "model_root = \"/datasets_1/sagarj/BellLabs/caffe_models/places/\"\n",
    "\n",
    "imagenet_mean = model_root + 'places205CNN_mean.binaryproto'\n",
    "\n",
    "meanFile = \"/datasets_1/sagarj/BellLabs/Data/places205CNN_mean.npy\"\n",
    "\n",
    "logfile = \"../Data/PlacesFeatExtractStreetview.txt\"\n",
    "\n",
    "\n",
    "#Size of images\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "NCHANNELS = 3\n",
    "meanR = 105.487823486\n",
    "meanG = 113.741088867\n",
    "meanB = 116.060394287\n",
    "\n",
    "meanMat = np.zeros((NCHANNELS,IMAGE_WIDTH,IMAGE_HEIGHT))\n",
    "meanMat[0,:,:].fill(meanR)\n",
    "meanMat[1,:,:].fill(meanG)\n",
    "meanMat[2,:,:].fill(meanB)\n",
    "#227\n",
    "\n",
    "def transform_img(img, img_width=IMAGE_WIDTH, img_height=IMAGE_HEIGHT):\n",
    "\n",
    "    #Histogram Equalization\n",
    "    img[:, :, 0] = cv2.equalizeHist(img[:, :, 0])\n",
    "    img[:, :, 1] = cv2.equalizeHist(img[:, :, 1])\n",
    "    img[:, :, 2] = cv2.equalizeHist(img[:, :, 2])\n",
    "\n",
    "    #Image Resizing\n",
    "    img = cv2.resize(img, (img_width, img_height), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def predictImage(imgPath , net, transformer):\n",
    "    \n",
    "\n",
    "    path = imgPath.strip()\n",
    "    im = caffe.io.load_image(path)\n",
    "    net.blobs['data'].data[...] = transformer.preprocess('data', im)\n",
    "    net.forward()\n",
    "    #out1 = net.blobs['prob'].data\n",
    "    out2 = net.blobs['fc7'].data\n",
    "    #print(out2.shape)\n",
    "    #out = np.concatenate((out1,out2.reshape(1,-1)),axis =1)\n",
    "    out = out2\n",
    "    #print(out.shape)\n",
    "    return out\n",
    "\n",
    "\n",
    "def getPlaces(Data ):\n",
    "    caffe.set_mode_gpu()    \n",
    "    model_def = model_root + 'places205CNN_deploy_upgraded.prototxt'#'test.prototxt'\n",
    "    model_weights = model_root +'places205CNN_iter_300000_upgraded.caffemodel'#'caffe_sentibank_train_iter_250000'\n",
    "    #model_def = model_root + 'places205VGG16/deploy_10.prototxt'\n",
    "    #model_weights = model_root +'places205VGG16/snapshot_iter_765280.caffemodel'\n",
    "    net = caffe.Net(model_def,      # defines the structure of the model\n",
    "                model_weights,  # contains the trained weights\n",
    "                caffe.TEST)     # use test mode (e.g., don't perform dropout)\n",
    "\n",
    "    net.blobs['data'].reshape(1,3,IMAGE_WIDTH,IMAGE_HEIGHT)\n",
    "    \n",
    "    transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape}) \n",
    "    transformer.set_transpose('data', (2,0,1))\n",
    "    transformer.set_channel_swap('data', (2,1,0))\n",
    "    transformer.set_raw_scale('data', 255.0)\n",
    "\n",
    "    for k in Data:\n",
    "        imgPath = Data[k]['path']\n",
    "        #print imgPath\n",
    "        path = imgPath.strip()\n",
    "        if not os.path.exists(path):\n",
    "            featArray = np.zeros((1,205))\n",
    "        else:\n",
    "            im = transformer.preprocess('data',caffe.io.load_image(path))\n",
    "            net.blobs['data'].data[...] = im - meanMat\n",
    "            net.forward()\n",
    "            #featArray = net.blobs['fc7'].data.copy()\n",
    "            featArray = np.squeeze(net.blobs['prob'].data.copy())\n",
    "        Data[k]['Places'] = featArray\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:47:04.282347Z",
     "start_time": "2018-06-11T19:45:16.524960Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "cityImages = getPlaces(cityImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T10:23:37.671893Z",
     "start_time": "2017-08-16T10:23:37.668520Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cityImages[cityImages.keys()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T10:14:27.480901Z",
     "start_time": "2017-08-16T10:14:27.474024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 67, 135,  60,  79,  92])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(cityImages[cityImages.keys()[1]]['Places'])[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T10:28:59.813149Z",
     "start_time": "2017-08-16T10:28:59.687544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k in cityImages:\n",
    "    top5idx = np.argsort(cityImages[k]['Places'])[-5:]\n",
    "    labels = [catDict[str(i)]['label'] for i in top5idx]\n",
    "    yays = set(labels).intersection(YesLabels)\n",
    "    nays = set(labels).intersection(NoLabels)\n",
    "    \n",
    "    if (len(yays) - len(nays)) > 0:\n",
    "        cityImages[k]['AugFlag'] = True\n",
    "        count +=1 \n",
    "    else:\n",
    "        cityImages[k]['AugFlag'] = False\n",
    "print count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LMDB for images that are positive for augmentatble labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T10:41:27.044543Z",
     "start_time": "2017-08-16T10:41:27.002620Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "import sys\n",
    "caffe_root = '/work/sagarj/caffe-rc5/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "import caffe\n",
    "from caffe.proto import caffe_pb2\n",
    "import lmdb\n",
    "\n",
    "#Size of images\n",
    "IMAGE_WIDTH = 227\n",
    "IMAGE_HEIGHT = 227\n",
    "\n",
    "def transform_img(img, img_width=IMAGE_WIDTH, img_height=IMAGE_HEIGHT):\n",
    "\n",
    "    #Histogram Equalization\n",
    "    img[:, :, 0] = cv2.equalizeHist(img[:, :, 0])\n",
    "    img[:, :, 1] = cv2.equalizeHist(img[:, :, 1])\n",
    "    img[:, :, 2] = cv2.equalizeHist(img[:, :, 2])\n",
    "\n",
    "    #Image Resizing\n",
    "    img = cv2.resize(img, (img_width, img_height), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def make_datum(img, label):\n",
    "    #image is numpy.ndarray format. BGR instead of RGB\n",
    "    return caffe_pb2.Datum(\n",
    "        channels=3,\n",
    "        width=IMAGE_WIDTH,\n",
    "        height=IMAGE_HEIGHT,\n",
    "        label=label,\n",
    "        data=np.rollaxis(img, 2).tostring())\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T10:33:51.235246Z",
     "start_time": "2017-08-16T10:33:51.230098Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testKeys = random.sample(list(cityImages.keys()), int(0.2*len(cityImages.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T10:34:17.255464Z",
     "start_time": "2017-08-16T10:34:17.180399Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainKeys = [k for k in cityImages.keys() if k not in testKeys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T10:34:32.716803Z",
     "start_time": "2017-08-16T10:34:32.711034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(632, 2532)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testKeys) , len(trainKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T10:32:15.512164Z",
     "start_time": "2017-08-16T10:32:15.507693Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_lmdb = '../Data/train_lmdb_beauty_validatedAugment'\n",
    "validation_lmdb = '../Data/validation_lmdb_beauty_validatedAugment'\n",
    "log = \"augValide.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T10:32:16.015101Z",
     "start_time": "2017-08-16T10:32:16.011822Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AugDir = \"/work/sagarj/Work/BellLabs/streetview/USAEasternAugImages/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T10:49:56.354253Z",
     "start_time": "2017-08-16T10:49:46.955969Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "train_data = {}\n",
    "for k in trainKeys:\n",
    "    src = [cityImages[k]['path']]\n",
    "    if cityImages[k]['AugFlag']:\n",
    "        if os.path.exists(AugDir + k):\n",
    "            images = os.listdir(AugDir + k)\n",
    "            random.shuffle(images)\n",
    "            for i in images:\n",
    "                src.append(AugDir + k + \"/\" + i)\n",
    "    train_data[k] = src\n",
    "        \n",
    "test_data = {}\n",
    "for k in testKeys:\n",
    "    src = [cityImages[k]['path']]\n",
    "    if cityImages[k]['AugFlag']:\n",
    "        if os.path.exists(AugDir + k):\n",
    "            images = os.listdir(AugDir + k)\n",
    "            random.shuffle(images)\n",
    "            for i in images:\n",
    "                src.append(AugDir + k + \"/\" + i)\n",
    "    test_data[k] = src\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T10:50:54.763895Z",
     "start_time": "2017-08-16T10:50:54.760388Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_data[train_data.keys()[101]]\n",
    "#cityImages[train_data.keys()[101]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-16T10:57:56.187742Z",
     "start_time": "2017-08-16T10:51:16.911414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train_lmdb\n",
      "\n",
      "Creating validation_lmdb\n",
      "\n",
      "Finished processing all images\n"
     ]
    }
   ],
   "source": [
    "# print ('Creating train_lmdb')\n",
    "# f = open(log,'w')\n",
    "# in_db = lmdb.open(train_lmdb, map_size=int(1e12))\n",
    "# with in_db.begin(write=True) as in_txn:\n",
    "#     for in_idx, (k, img_path) in enumerate(train_data.items()):\n",
    "#         label = cityImages[k]['label']\n",
    "#         for p in img_path:\n",
    "#             img = cv2.imread(p, cv2.IMREAD_COLOR)\n",
    "#             img = transform_img(img, img_width=IMAGE_WIDTH, img_height=IMAGE_HEIGHT)\n",
    "#             datum = make_datum(img, label)\n",
    "#             in_txn.put('{:0>5d}'.format(in_idx), datum.SerializeToString())\n",
    "#             line = '{:0>5d}'.format(in_idx) + ':' + p + \"\\n\"\n",
    "#             f.write(line)\n",
    "# in_db.close()\n",
    "\n",
    "\n",
    "# print ('\\nCreating validation_lmdb')\n",
    "\n",
    "# in_db = lmdb.open(validation_lmdb, map_size=int(1e12))\n",
    "# with in_db.begin(write=True) as in_txn:\n",
    "#     for in_idx, (k, img_path) in enumerate(test_data.items()):\n",
    "#         label = cityImages[k]['label']\n",
    "#         for p in img_path:\n",
    "#             img = cv2.imread(p, cv2.IMREAD_COLOR)\n",
    "#             img = transform_img(img, img_width=IMAGE_WIDTH, img_height=IMAGE_HEIGHT)\n",
    "#             datum = make_datum(img, label)\n",
    "#             in_txn.put('{:0>5d}'.format(in_idx), datum.SerializeToString())\n",
    "#             line = '{:0>5d}'.format(in_idx) + ':' + p + \"\\n\"\n",
    "#             f.write(line)\n",
    "# in_db.close()\n",
    "# f.close()\n",
    "# print ('\\nFinished processing all images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
